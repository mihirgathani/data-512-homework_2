# **Data 512: HW 2 - Considering Bias in Data**

## Project Goal
The goal of this assignment is to explore the concept of bias in data using Wikipedia articles. This assignment will consider articles on political figures from different countries. For this assignment, we will combine a dataset of Wikipedia articles with a dataset of country populations, and use a machine learning service called ORES to estimate the quality of each article. We will then perform an analysis of how the coverage of politicians on Wikipedia and the quality of articles about politicians varies among countries.

The analysis will consist of a series of tables that show:
- The countries with the greatest and least coverage of politicians on Wikipedia compared to their population.
- The countries with the highest and lowest proportion of high quality articles about politicians.
- A ranking of geographic regions by articles-per-person and proportion of high quality articles.

## Licenses

**Source Data**

1. [**politicians_by_country.csv**](Resources/politicians_by_country_AUG.2024.csv): This file contains a list of Wikipedia articles about politicians from various countries. It was generated by crawling the Wikipedia [Category:Politicians by nationality](https://en.wikipedia.org/wiki/Category:Politicians_by_nationality) page to create a dataset of politician articles along with related country information.

**Note:** It is also subject to the [Wikimedia Foundation Terms of Use](https://foundation.wikimedia.org/wiki/Policy:Terms_of_Use). In accordance with these terms, the dataset created and used in this project is for research and analytical purposes. I have ensured this data should comply with the Wikimedia Foundation's policies regarding data usage, privacy, and attribution.

2. [**population_by_country_AUG.2024.csv**](./Resources/population_by_country_AUG.2024.csv): This dataset contains population data sourced from the Population Reference Bureau’s [world population data sheet](https://www.prb.org/international/indicator/population/table). The file includes population counts for individual countries and cumulative totals for regions. 

**Data Acquisition Code**

- Parts of the code in ["Wikipedia_Politician_Bias_Analysis.ipynb"](./Wikipedia_Politician_Bias_Analysis.ipynb) file were taken as-is or with minimal changes from the [example code](./Resources/) that was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.3 - August 16, 2024


## API Documentation
This project uses the following Wikimedia APIs for data collection:

- [**MediaWiki PageInfo API**](https://www.mediawiki.org/wiki/API:Info): This API is used to fetch page info for a given article title.

- [**ORES API**](https://www.mediawiki.org/wiki/ORES): This API is used to fetch article rating, given article's revision id.


## Steps to run the code
In order to gather the data and create the final data tables, you would have to run the following steps:

- **Pre-run Information:** In order to successfully run the code, you need to clone/ download the repository and upload it to a Jupyter Environment (a place where .ipynb files can run). After this, you will need to ensure that the required Python packages are installed. Instructions to do so can be found in the individual code files.

Now that you have the entire repository cloned and environment setup, you can follow the steps:

1. First, you need to run the ["Wikipedia_Politician_Bias_Analysis.ipynb"](./Wikipedia_Politician_Bias_Analysis.ipynb) file. In order to do this, you need to click on run all cells or restart kernal and run all cells button in the jupyter notebook. 
**Note-** Since the data acquisition process calls the API to get the required data, this file would take about 2-2.5 hours to run, store the results, and create the required tables. To aid in speeding up the process, you can find the data from the API stored in the [Generated_Data_Files folder](./Generated_Data_Files/). 

Additionally, you would require API access to run the code. Instructions for the same are available inside the code file.

## Data Files
During the execution of the project, several data files are created. Below is a list of these files and their descriptions:

1. [`all_articles_pageinfo.json`](./Generated_Data_Files/all_articles_pageinfo.json): This file contains the revision id and other information all the politician articles

2. [`all_articles_ores_scores.csv`](./Generated_Data_Files/all_articles_ores_scores.csv): This file contains the ORES scores that were fetched from the API using the revision id's from the previous file.

3. [`wp_countries-no_match.txt`](./Generated_Data_Files/wp_countries-no_match.txt): This file contains all countries for which there are no matches, i.e. when we merge the wikipedia data and population data together these countries are not present is either.

4. [`wp_politicians_by_country.csv`](./Generated_Data_Files/wp_politicians_by_country.csv): This file contains the combined data on politicians, countries, and article quality.

This CSV will include the following columns:

   - **country**: The name of the country.
   - **region**: The geographical region to which the country belongs.
   - **population**: The population of the country.
   - **article_title**: The title of the politician’s Wikipedia article.
   - **revision_id**: The latest revision ID of the article.
   - **article_quality**: The quality rating of the article assigned by ORES.

## Final Output
The analysis yields six data tables, showcasing:

1. The top 10 countries with the highest total articles per capita.
2. The bottom 10 countries with the lowest total articles per capita.
3. The top 10 countries with the highest high-quality articles per capita.
4. The bottom 10 countries with the lowest high-quality articles per capita.
5. A ranked list of geographic regions by total articles per capita.
6. A ranked list of geographic regions by high-quality articles per capita.

These tables are embedded in the ["Wikipedia_Politician_Bias_Analysis.ipynb"](./Wikipedia_Politician_Bias_Analysis.ipynb) file. 

## Research Implications
Through this assignment, I have gained a deeper understanding of the complexities surrounding data biases, especially in the context of information sourced from platforms like Wikipedia. One of the key findings was the unexpected absence of expected biases toward developed nations regarding high-quality article representation. Instead, the data revealed significant selection biases, particularly the absence of contributions from notable countries such as the United States, Canada, and Australia. This discrepancy surprised me, as I initially assumed that higher-quality articles would correlate directly with these countries' well-established educational and technological infrastructures. Moreover, this experience underscored the critical need to recognize inherent biases within source data, which can arise from demographics, cultural influences, and literacy levels. These factors not only affect the availability of quality information but also highlight the importance of documentation and reproducibility in data science projects.

**What might your results suggest about (English) Wikipedia as a data source?**  
- The focus on English content skews the representation of global knowledge, making it difficult to draw accurate conclusions about topics in non-English-speaking regions. This has broader implications for how the internet shapes knowledge and societal perspectives, as the information may not accurately reflect the diversity of global views.

**What might your results suggest about the internet and global society in general?**  
- The results highlight significant societal disparities in global access to information and technology. Despite the internet's apparent openness, infrastructure, education, and cultural influences create notable gaps. Topics receiving more attention often align with regions or populations that have better access to educational resources and platforms for contribution, reflecting broader societal biases where certain voices dominate global conversations. Additionally, this could also affect how many articles from that source are rated.

**How might a researcher supplement or transform this dataset to potentially correct for the limitations/biases you observed?**  
- To enhance the dataset and address identified biases, researchers could integrate additional demographic and socio-economic data from international organizations like the World Bank or United Nations. This supplementary information would provide a richer context for understanding the factors influencing article creation and quality. Furthermore, incorporating qualitative insights through surveys or interviews with Wikipedia contributors from various backgrounds could illuminate their motivations and challenges, fostering a more nuanced understanding of the content landscape.

## Potential Considerations: 
The following considerations are important to consider when using this code:

- **API Rate Limits**: Although the code accounts for API rate limits, there might be a possibility that it may affect data acquistion, especially in case the API changes. For me, 4 requests failed for the pageinfo and some more failed when called the ORES API. 

- **API Changes:** There is a possibility that the Wikimedia API may undergo changes over time causing inconsistency in data, and potentially different results. Users of this project should check for any updates to the API that might affect data acquisition or analysis methods.

- **Article Title Changed:** In case an article is renamed, or updated in a wrong way, it may lead to data inconsistencies. This is possible since Wikipedia relies on it's users for the data.

*Note: Used ChatGPT to help with some text in research implications*
